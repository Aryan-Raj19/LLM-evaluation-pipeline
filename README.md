# LLM Response Evaluation Pipeline


## Overview
This project evaluates AI-generated responses in real time using conversation history and retrieved context documents.


## Metrics
- Response Relevance & Completeness
- Hallucination / Faithfulness
- Latency & Cost Estimation


## Local Setup
```bash
pip install -r requirements.txt
python evaluator.py
```
